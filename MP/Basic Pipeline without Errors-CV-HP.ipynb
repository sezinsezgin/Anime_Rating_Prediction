{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imdb \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "anime_data = pd.read_csv(\"new_anime_data1.csv\")\n",
    "\n",
    "anime_data['episodes'] = anime_data['episodes'].replace('Unknown', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "\n",
    "\n",
    "anime_data['genre'].fillna('',  inplace=True)\n",
    "anime_data['overview'].fillna('',  inplace=True)\n",
    "anime_data['type'].fillna('',  inplace=True)\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['genre']\n",
    "    if(pd.isnull(item)):\n",
    "            item =\"\"\n",
    "    else:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            item = ','.join(item)\n",
    "        else:\n",
    "            item = item.replace(\" \",\"\")\n",
    "            item = item.replace(\"[\",\"\")\n",
    "            item = item.replace(\"]\",\"\")\n",
    "            item = item.replace(\"'\",\"\")\n",
    "    l1.append(item) \n",
    "      \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['overview']\n",
    "    if(pd.isnull(row['overview'])):\n",
    "            item = \"\"\n",
    "    else:\n",
    "       # for item in anime_data['overview']:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                item = ','.join(item)\n",
    "            else:\n",
    "                item = item.replace(\"[\",\"\")\n",
    "                item = item.replace(\"]\",\"\")\n",
    "    l2.append(item) \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['type']\n",
    "    if(pd.isnull(row['type'])):\n",
    "        item = np.nan\n",
    "    else:    \n",
    "        if \"movie\" in item:\n",
    "            item = \"movie\"\n",
    "        else:\n",
    "            item = \"tv series\"\n",
    "    l3.append(item)   \n",
    "    \n",
    "    \n",
    "anime_data['genre'] = l1   \n",
    "anime_data['overview'] = l2\n",
    "anime_data['type'] = l3\n",
    "\n",
    "#drop dublicate\n",
    "anime_data.drop_duplicates(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train, anime_test = train_test_split(anime_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "def get_words(x):\n",
    "    bagofwords=[]\n",
    "    for i in x:\n",
    "        if i[1]=='NN':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNP':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNPS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJ':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RB':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBS':\n",
    "            bagofwords.append(i[0])\n",
    "    return bagofwords\n",
    "\n",
    "def clean_words(x):\n",
    "    b=nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    result=get_words(b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transformation(dataset):\n",
    "    #Hot_Encoding\n",
    "    df = dataset.genre.str.get_dummies(',')\n",
    "    \n",
    "    type_lb = LabelBinarizer()\n",
    "    X = type_lb.fit_transform(dataset.type.values)\n",
    "\n",
    "    dfOneHot = pd.DataFrame(X, columns = [\"movie/TVseries\" for i in range(X.shape[1])])\n",
    "    dataset = pd.concat([dataset, dfOneHot], axis=1)\n",
    "    dataset = pd.concat([dataset, df], axis=1)\n",
    "\n",
    "    dataset['movie/TVseries'].fillna(0, inplace=True)\n",
    "    \n",
    "    #Bag of Words\n",
    "    summary_doc = dataset['overview'].fillna(\"\").map(clean_words)\n",
    "    summary_doc =summary_doc.apply(','.join)\n",
    " \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    overview_feature = vectorizer.fit_transform(summary_doc).toarray()\n",
    "\n",
    "    df = pd.DataFrame(overview_feature, columns = [\"word\"+ str(int(i)) for i in range(overview_feature.shape[1])])\n",
    "    dataset = pd.concat([dataset, df], axis=1)\n",
    "    \n",
    "    dataset = dataset.drop(columns=['Unnamed: 0', 'anime_id', 'name', 'genre', 'overview', 'type'])\n",
    "    \n",
    "    #drop NaN values\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "anime_train = feature_transformation(anime_train)\n",
    "anime_test = feature_transformation(anime_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1481, 7136)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 3770)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train_columns = list(anime_train)\n",
    "anime_test_columns = list(anime_test)\n",
    "\n",
    "for col_name in anime_train_columns:\n",
    "    if col_name not in anime_test_columns:\n",
    "        anime_test[str(col_name)] = 0\n",
    "        \n",
    "for col_name in anime_test_columns:\n",
    "    if col_name not in anime_train_columns:\n",
    "        anime_test.drop([str(col_name)], axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481, 7136)\n",
      "(143, 7136)\n"
     ]
    }
   ],
   "source": [
    "print(anime_train.shape)\n",
    "print(anime_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_y_train = anime_train['rating']\n",
    "anime_X_train = anime_train.drop(columns=['rating'])\n",
    "\n",
    "anime_y_test = anime_test['rating']\n",
    "anime_X_test = anime_test.drop(columns=['rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481, 700)\n",
      "(143, 700)\n",
      "(1481, 700)\n",
      "(143, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression,k=700)#anime_X_test.shape[1]-1)\n",
    "features = selector.fit(anime_X_train, anime_y_train)\n",
    "\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "#print(fit.scores_)\n",
    "\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "anime_X_train = features.transform(anime_X_train)\n",
    "anime_X_test = features.transform(anime_X_test)\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "anime_X_train = scaler.fit_transform(anime_X_train)  \n",
    "anime_X_test = scaler.transform(anime_X_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataset, label):\n",
    "    clf = model\n",
    "    clf.fit(dataset, label)\n",
    "    return clf\n",
    "\n",
    "def testing_evaluation(model, testset):\n",
    "    # Make predictions using the testing set\n",
    "    anime_y_pred = model.predict(testset)\n",
    "\n",
    "    # The mean absolute error\n",
    "    print(\"Mean absolute error: %.2f\" % np.sqrt(mean_absolute_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(anime_y_test, anime_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 8534622.07\n",
      "Mean squared error: 186476490319156.09\n",
      "Variance score: -32117493953371937229542260736.00\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.LinearRegression(), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.85\n",
      "Mean squared error: 0.97\n",
      "Variance score: 0.13\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=0.1), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create a list of alphas to cross-validate against\\nalphas = np.logspace(-10, 1, 100)\\n\\n# Instantiate the linear model and visualizer\\nmodel = LassoCV(alphas=alphas, cv = 5)\\nvisualizer = AlphaSelection(model)\\n\\nvisualizer.fit(anime_X_train, anime_y_train)\\ng = visualizer.poof()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create a list of alphas to cross-validate against\n",
    "alphas = np.logspace(-10, 1, 100)\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = LassoCV(alphas=alphas, cv = 5)\n",
    "visualizer = AlphaSelection(model)\n",
    "\n",
    "visualizer.fit(anime_X_train, anime_y_train)\n",
    "g = visualizer.poof()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.974 0.824 0.867 0.863 1.068 0.953 0.951 0.793 1.033 0.91 ]\n",
      "Mean: 0.9236783486943997\n",
      "Standard deviation: 0.08419967263611836\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "scores = cross_val_score(clf, anime_X_train, anime_y_train, scoring=\"neg_mean_squared_error\", cv=10) \n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search For Hyper Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\master\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters are:  {'alpha': 0.01}\n",
      "The mean squared Error is: 0.81\n",
      "best estimator is:  Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "def checkHP(model, folds, dataset, label):\n",
    "    alphas = np.logspace(-5, 2, 30)\n",
    "    parameters = {\n",
    "                   \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  }\n",
    "\n",
    "    gd_sr = GridSearchCV(estimator=model,  \n",
    "                         param_grid=parameters,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=folds)\n",
    "\n",
    "    gd_sr.fit(dataset, label)  \n",
    "    \n",
    "    best_parameters = gd_sr.best_params_  \n",
    "    print(\"best parameters are: \", best_parameters)\n",
    "\n",
    "    best_result = gd_sr.best_score_  \n",
    "    print(\"The mean squared Error is: %.2f\" % -best_result) \n",
    "    best_estimator = gd_sr.best_estimator_\n",
    "    print(\"best estimator is: \", best_estimator) \n",
    "    \n",
    "checkHP(clf, 10, anime_X_train, anime_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.88\n",
      "Mean squared error: 1.04\n",
      "Variance score: -0.00\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "   normalize=False, positive=False, precompute=False, random_state=None,\n",
    "   selection='cyclic', tol=0.0001, warm_start=False), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
