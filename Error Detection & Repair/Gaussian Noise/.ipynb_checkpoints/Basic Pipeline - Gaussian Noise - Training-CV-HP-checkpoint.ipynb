{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imdb \n",
    "import pandas as pd\n",
    "import random\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from matplotlib import pyplot\n",
    "#from error_generator import Explicit_Missing_Value\n",
    "#from error_generator import Implicit_Missing_Value\n",
    "#from error_generator import White_Noise\n",
    "from error_generator import Gaussian_Noise\n",
    "from error_generator import Random_Active_Domain\n",
    "from error_generator import Similar_Based_Active_Domain\n",
    "from error_generator import Typo_Keyboard\n",
    "from error_generator import Typo_Butterfingers\n",
    "from error_generator import Word2vec_Nearest_Neighbor\n",
    "from error_generator import Value_Selector\n",
    "from error_generator import List_selected\n",
    "from error_generator import Read_Write\n",
    "from error_generator import Error_Generator\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "anime_data = pd.read_csv(\"../../new_anime_data1.csv\", index_col=0)\n",
    "\n",
    "anime_data['episodes'] = anime_data['episodes'].replace('Unknown', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "\n",
    "\n",
    "anime_data['genre'].fillna('',  inplace=True)\n",
    "anime_data['overview'].fillna('',  inplace=True)\n",
    "anime_data['type'].fillna('',  inplace=True)\n",
    "anime_data['episodes'].fillna(0,  inplace=True)\n",
    "anime_data['rating'].fillna(0,  inplace=True)\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['genre']\n",
    "    if(pd.isnull(item)):\n",
    "            item =\"\"\n",
    "    else:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            item = ','.join(item)\n",
    "        else:\n",
    "            item = item.replace(\" \",\"\")\n",
    "            item = item.replace(\"[\",\"\")\n",
    "            item = item.replace(\"]\",\"\")\n",
    "            item = item.replace(\"'\",\"\")\n",
    "    l1.append(item) \n",
    "      \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['overview']\n",
    "    if(pd.isnull(row['overview'])):\n",
    "            item = \"\"\n",
    "    else:\n",
    "       # for item in anime_data['overview']:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                item = ','.join(item)\n",
    "            else:\n",
    "                item = item.replace(\"[\",\"\")\n",
    "                item = item.replace(\"]\",\"\")\n",
    "    l2.append(item) \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['type']\n",
    "    if(pd.isnull(row['type'])):\n",
    "        item = np.nan\n",
    "    else:    \n",
    "        if \"movie\" in item:\n",
    "            item = \"movie\"\n",
    "        else:\n",
    "            item = \"tv series\"\n",
    "    l3.append(item)\n",
    "\n",
    "    \n",
    "    \n",
    "anime_data['genre'] = l1   \n",
    "anime_data['overview'] = l2\n",
    "anime_data['type'] = l3\n",
    "\n",
    "#drop dublicate\n",
    "anime_data.drop_duplicates(inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anime_train, anime_test = train_test_split(anime_data, test_size=0.2)\n",
    "anime_test.to_csv(\"testDataset.csv\", index = False,\n",
    "                  columns = ['anime_id', 'name','genre','type', 'episodes','rating', 'members', 'overview'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of the data before adding Gaussian Noise\n",
      "7128       864\n",
      "3238      1511\n",
      "5904     51170\n",
      "2739     32040\n",
      "11093     4481\n",
      "9947       120\n",
      "11946     1137\n",
      "3548      1667\n",
      "3715      5949\n",
      "Name: members, dtype: int64\n",
      "\n",
      "sample of the data after adding Gaussian Noise\n",
      "4537      4504.439918\n",
      "7128     -5217.510376\n",
      "3238      1911.987604\n",
      "5904     60642.868564\n",
      "2739     29160.784281\n",
      "11093     2795.890546\n",
      "9947    -12550.795994\n",
      "11946    16078.933081\n",
      "3548       995.704019\n",
      "3715     -1631.109181\n",
      "Name: members, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3267: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "sample = anime_train.members[0:600]\n",
    "print(\"sample of the data before adding Gaussian Noise\")\n",
    "print(sample[1:10])\n",
    "mu, sigma = 0, 10000\n",
    "\n",
    "noise = np.random.normal(mu, sigma, 600)\n",
    "\n",
    "signal = sample + noise\n",
    "\n",
    "print(\"\\nsample of the data after adding Gaussian Noise\")\n",
    "print(signal[0:10])\n",
    "   \n",
    "anime_train.members.update(signal)\n",
    "\n",
    "anime_train.to_csv(\"dirtyTrainDataset.csv\", index = False,\n",
    "                  columns = ['anime_id', 'name','genre','type', 'episodes','rating', 'members', 'overview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18ebc030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sample.sort_values(), signal.sort_values(), linewidth=1, linestyle=\"-\", c=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "def get_words(x):\n",
    "    bagofwords=[]\n",
    "    for i in x:\n",
    "        if i[1]=='NN':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNP':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNPS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJ':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RB':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBS':\n",
    "            bagofwords.append(i[0])\n",
    "    return bagofwords\n",
    "\n",
    "def clean_words(x):\n",
    "    b=nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    result=get_words(b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(train_data_set, test_data_set):\n",
    "    train_dummies = train_data_set.genre.str.get_dummies(',')\n",
    "    test_dummies = test_data_set.genre.str.get_dummies(',')\n",
    "    \n",
    "    #print(\"Train Dummies\",train_dummies.shape)    \n",
    "    #print(\"Test Dummies\",test_dummies.shape)\n",
    "    \n",
    "    #### ALÄ°GN\n",
    "    train_dummies, test_dummies = train_dummies.align(test_dummies, axis=1, join='left')\n",
    "    \n",
    "    test_dummies.fillna(0, inplace=True)\n",
    "    \n",
    "    type_lb = LabelBinarizer()\n",
    "    fitted_type_lb = type_lb.fit(train_data_set.type.values)\n",
    "    X_train = type_lb.transform(train_data_set.type.values)\n",
    "    X_test  = type_lb.transform(test_data_set.type.values)\n",
    "    \n",
    "    dfOneHot_train = pd.DataFrame(X_train, columns = [\"movie/TVseries\" for i in range(X_train.shape[1])])\n",
    "    dfOneHot_test  = pd.DataFrame(X_test,  columns = [\"movie/TVseries\" for i in range(X_test.shape[1])])\n",
    "    \n",
    "    \n",
    "    train_data_set = pd.concat([train_data_set, dfOneHot_train], axis=1, join=\"inner\")\n",
    "    train_data_set = pd.concat([train_data_set, train_dummies ], axis=1, join=\"inner\")\n",
    "\n",
    "    test_data_set = pd.concat([test_data_set, dfOneHot_test], axis=1)\n",
    "    test_data_set = pd.concat([test_data_set, test_dummies],  axis=1)\n",
    "    \n",
    "    test_data_set['movie/TVseries'].fillna(0, inplace=True)\n",
    "    train_data_set['movie/TVseries'].fillna(0, inplace=True)\n",
    "\n",
    "    return ([train_data_set, test_data_set])\n",
    "\n",
    "def feature_transformation(train_data_set, test_data_set):\n",
    "    \n",
    "    dummieset = get_dummies(train_data_set, test_data_set)\n",
    "    train_data_set = dummieset[0]\n",
    "    test_data_set = dummieset[1]\n",
    "        \n",
    "    #Bag of Words\n",
    "    summary_doc_train = train_data_set['overview'].fillna(\"\").map(clean_words)\n",
    "    summary_doc_train =summary_doc_train.apply(','.join)\n",
    "    \n",
    "    summary_doc_test = test_data_set['overview'].fillna(\"\").map(clean_words)\n",
    "    summary_doc_test =summary_doc_test.apply(','.join)\n",
    " \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    fitted_vectorizer = vectorizer.fit(summary_doc_train)\n",
    "    overview_feature_train = fitted_vectorizer.transform(summary_doc_train).toarray()\n",
    "    overview_feature_test = fitted_vectorizer.transform(summary_doc_test).toarray()\n",
    "\n",
    "    df_train = pd.DataFrame(overview_feature_train, columns = [\"word\"+ str(int(i)) for i in range(overview_feature_train.shape[1])])\n",
    "    train_data_set = pd.concat([train_data_set, df_train], axis=1)\n",
    "    \n",
    "    df_test = pd.DataFrame(overview_feature_test, columns = [\"word\"+ str(int(i)) for i in range(overview_feature_test.shape[1])])\n",
    "    test_data_set = pd.concat([test_data_set, df_test], axis=1)\n",
    "    \n",
    "    train_data_set = train_data_set.drop(columns=['anime_id', 'name', 'genre', 'overview', 'type'])\n",
    "    test_data_set = test_data_set.drop(columns=['anime_id', 'name', 'genre', 'overview', 'type'])\n",
    "    \n",
    "    #drop NaN values\n",
    "    train_data_set.dropna(inplace=True)\n",
    "    test_data_set.dropna(inplace=True)\n",
    "    \n",
    "    train_data_set.fillna(0, inplace=True)\n",
    "    test_data_set.fillna(0, inplace=True)\n",
    "    \n",
    "    return ([train_data_set, test_data_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "transformed_features = feature_transformation(anime_train, anime_test)\n",
    "\n",
    "anime_train = transformed_features[0]\n",
    "anime_test = transformed_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2453)\n",
      "(27, 2453)\n"
     ]
    }
   ],
   "source": [
    "#anime_train = anime_train.dropna()\n",
    "print(anime_train.shape)\n",
    "print(anime_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_y_train = anime_train['rating']\n",
    "anime_X_train = anime_train.drop(columns=['rating'])\n",
    "\n",
    "anime_y_test = anime_test['rating']\n",
    "anime_X_test = anime_test.drop(columns=['rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2452)\n",
      "(27, 2452)\n",
      "(32, 700)\n",
      "(27, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\feature_selection\\univariate_selection.py:299: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression,k=700)\n",
    "features = selector.fit(anime_X_train, anime_y_train)\n",
    "\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "#print(fit.scores_)\n",
    "\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "anime_X_train = features.transform(anime_X_train)\n",
    "anime_X_test = features.transform(anime_X_test)\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "anime_X_train = scaler.fit_transform(anime_X_train)  \n",
    "anime_X_test = scaler.transform(anime_X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataset, label):\n",
    "    clf = model\n",
    "    clf.fit(dataset, label)\n",
    "    return clf\n",
    "\n",
    "def testing_evaluation(model, testset):\n",
    "    # Make predictions using the testing set\n",
    "    anime_y_pred = model.predict(testset)\n",
    "\n",
    "    # The mean absolute error\n",
    "    print(\"Mean absolute error: %.2f\" % np.sqrt(mean_absolute_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(anime_y_test, anime_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.64\n",
      "Mean squared error: 0.50\n",
      "Variance score: -6.59\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.LinearRegression(), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.73\n",
      "Mean squared error: 0.56\n",
      "Variance score: -8.37\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=0.1), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create a list of alphas to cross-validate against\\nalphas = np.logspace(-10, 1, 100)\\n\\n# Instantiate the linear model and visualizer\\nmodel = LassoCV(alphas=alphas, cv = 5)\\nvisualizer = AlphaSelection(model)\\n\\nvisualizer.fit(anime_X_train, anime_y_train)\\ng = visualizer.poof()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create a list of alphas to cross-validate against\n",
    "alphas = np.logspace(-10, 1, 100)\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = LassoCV(alphas=alphas, cv = 5)\n",
    "visualizer = AlphaSelection(model)\n",
    "\n",
    "visualizer.fit(anime_X_train, anime_y_train)\n",
    "g = visualizer.poof()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.426 0.076 0.101 0.174 0.223]\n",
      "Mean: 0.20015489883566645\n",
      "Standard deviation: 0.12447324868607493\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "scores = cross_val_score(clf, anime_X_train, anime_y_train, scoring=\"neg_mean_squared_error\", cv=5) \n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search For Hyper Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters are:  {'alpha': 0.1}\n",
      "The mean squared Error is: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "c:\\users\\larat\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pattern-3.6-py3.7.egg\\pattern\\text\\en\\..\\..\\..\\..\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def checkHP(model, folds, dataset, label):\n",
    "    parameters = {\n",
    "                   \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  }\n",
    "\n",
    "    gd_sr = GridSearchCV(estimator=model,  \n",
    "                         param_grid=parameters,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=folds)\n",
    "\n",
    "    gd_sr.fit(dataset, label)  \n",
    "    \n",
    "    best_parameters = gd_sr.best_params_  \n",
    "    print(\"best parameters are: \", best_parameters)\n",
    "\n",
    "    best_result = gd_sr.best_score_  \n",
    "    print(\"The mean squared Error is: %.2f\" % -best_result) \n",
    "    \n",
    "checkHP(clf, 5, anime_X_train, anime_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.73\n",
      "Mean squared error: 0.56\n",
      "Variance score: -8.37\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=0.1), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
