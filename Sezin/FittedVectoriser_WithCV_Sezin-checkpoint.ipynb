{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imdb \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "anime_data = pd.read_csv(\"/Users/sezin/PycharmProjects/dsa-DC4ML/new_anime_data1.csv\")\n",
    "\n",
    "anime_data['episodes'] = anime_data['episodes'].replace('Unknown', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "\n",
    "\n",
    "anime_data['genre'].fillna('',  inplace=True)\n",
    "anime_data['overview'].fillna('',  inplace=True)\n",
    "anime_data['type'].fillna('',  inplace=True)\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['genre']\n",
    "    if(pd.isnull(item)):\n",
    "            item =\"\"\n",
    "    else:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            item = ','.join(item)\n",
    "        else:\n",
    "            item = item.replace(\" \",\"\")\n",
    "            item = item.replace(\"[\",\"\")\n",
    "            item = item.replace(\"]\",\"\")\n",
    "            item = item.replace(\"'\",\"\")\n",
    "    l1.append(item) \n",
    "      \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['overview']\n",
    "    if(pd.isnull(row['overview'])):\n",
    "            item = \"\"\n",
    "    else:\n",
    "       # for item in anime_data['overview']:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                item = ','.join(item)\n",
    "            else:\n",
    "                item = item.replace(\"[\",\"\")\n",
    "                item = item.replace(\"]\",\"\")\n",
    "    l2.append(item) \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['type']\n",
    "    if(pd.isnull(row['type'])):\n",
    "        item = np.nan\n",
    "    else:    \n",
    "        if \"movie\" in item:\n",
    "            item = \"movie\"\n",
    "        else:\n",
    "            item = \"tv series\"\n",
    "    l3.append(item)   \n",
    "    \n",
    "    \n",
    "anime_data['genre'] = l1   \n",
    "anime_data['overview'] = l2\n",
    "anime_data['type'] = l3\n",
    "\n",
    "#drop dublicate\n",
    "anime_data.drop_duplicates(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train, anime_test = train_test_split(anime_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "def get_words(x):\n",
    "    bagofwords=[]\n",
    "    for i in x:\n",
    "        if i[1]=='NN':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNP':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNPS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJ':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RB':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBS':\n",
    "            bagofwords.append(i[0])\n",
    "    return bagofwords\n",
    "\n",
    "def clean_words(x):\n",
    "    b=nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    result=get_words(b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(train_data_set, test_data_set):\n",
    "    train_dummies = train_data_set.genre.str.get_dummies(',')\n",
    "    test_dummies = test_data_set.genre.str.get_dummies(',')\n",
    "    \n",
    "    print(\"Train Dummies\",train_dummies.shape)    \n",
    "    print(\"Test Dummies\",test_dummies.shape)\n",
    "    \n",
    "    #### ALÄ°GN\n",
    "    train_dummies, test_dummies = train_dummies.align(test_dummies, axis=1, join='left')\n",
    "    test_dummies.fillna(0, inplace=True)\n",
    "    \n",
    "    type_lb = LabelBinarizer()\n",
    "    fitted_type_lb = type_lb.fit(train_data_set.type.values)\n",
    "    X_train = fitted_type_lb.transform(train_data_set.type.values)\n",
    "    X_test = fitted_type_lb.transform(test_data_set.type.values)\n",
    "    \n",
    "    dfOneHot_train = pd.DataFrame(X_train, columns = [\"movie/TVseries\" for i in range(X_train.shape[1])])\n",
    "    dfOneHot_test = pd.DataFrame(X_test, columns = [\"movie/TVseries\" for i in range(X_test.shape[1])])\n",
    "    \n",
    "    train_data_set = pd.concat([train_data_set, dfOneHot_train], axis=1)\n",
    "    train_data_set = pd.concat([train_data_set, train_dummies], axis=1)\n",
    "\n",
    "    test_data_set = pd.concat([test_data_set, dfOneHot_test], axis=1)\n",
    "    test_data_set = pd.concat([test_data_set, test_dummies], axis=1)\n",
    "    \n",
    "    test_data_set['movie/TVseries'].fillna(0, inplace=True)\n",
    "    train_data_set['movie/TVseries'].fillna(0, inplace=True)\n",
    "\n",
    "    return([train_data_set, test_data_set])\n",
    "\n",
    "def feature_transformation(train_data_set, test_data_set):\n",
    "    \n",
    "    dummieset = get_dummies(train_data_set, test_data_set)\n",
    "    train_data_set = dummieset[0]\n",
    "    test_data_set = dummieset[1]\n",
    "        \n",
    "    #Bag of Words\n",
    "    summary_doc_train = train_data_set['overview'].fillna(\"\").map(clean_words)\n",
    "    summary_doc_train =summary_doc_train.apply(','.join)\n",
    "    \n",
    "    summary_doc_test = test_data_set['overview'].fillna(\"\").map(clean_words)\n",
    "    summary_doc_test =summary_doc_test.apply(','.join)\n",
    " \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    fitted_vectorizer = vectorizer.fit(summary_doc_train)\n",
    "    overview_feature_train = fitted_vectorizer.transform(summary_doc_train).toarray()\n",
    "    overview_feature_test = fitted_vectorizer.transform(summary_doc_test).toarray()\n",
    "\n",
    "    df_train = pd.DataFrame(overview_feature_train, columns = [\"word\"+ str(int(i)) for i in range(overview_feature_train.shape[1])])\n",
    "    train_data_set = pd.concat([train_data_set, df_train], axis=1)\n",
    "    \n",
    "    df_test = pd.DataFrame(overview_feature_test, columns = [\"word\"+ str(int(i)) for i in range(overview_feature_test.shape[1])])\n",
    "    test_data_set = pd.concat([test_data_set, df_test], axis=1)\n",
    "    \n",
    "    train_data_set = train_data_set.drop(columns=['Unnamed: 0', 'anime_id', 'name', 'genre', 'overview', 'type'])\n",
    "    test_data_set = test_data_set.drop(columns=['Unnamed: 0', 'anime_id', 'name', 'genre', 'overview', 'type'])\n",
    "    \n",
    "    #drop NaN values\n",
    "    train_data_set.dropna(inplace=True)\n",
    "    test_data_set.dropna(inplace=True)\n",
    "    \n",
    "    #train_data_set.fillna(0, inplace=True)\n",
    "    #test_data_set.fillna(0, inplace=True)\n",
    "    \n",
    "    return ([train_data_set, test_data_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dummies (1600, 57)\n",
      "Test Dummies (400, 51)\n",
      "(1988, 7122)\n",
      "(989, 7122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "transformed_features = feature_transformation(anime_train, anime_test)\n",
    "\n",
    "anime_train = transformed_features[0]\n",
    "anime_test = transformed_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1488, 7122)\n",
      "Test (138, 7122)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\", anime_train.shape)\n",
    "print(\"Test\", anime_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anime_train_columns = list(anime_train)\\nanime_test_columns = list(anime_test)\\n\\nfor col_name in anime_train_columns:\\n    if col_name not in anime_test_columns:\\n        anime_test[str(col_name)] = 0\\n        \\nfor col_name in anime_test_columns:\\n    if col_name not in anime_train_columns:\\n        anime_test.drop([str(col_name)], axis=1, inplace=True)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"anime_train_columns = list(anime_train)\n",
    "anime_test_columns = list(anime_test)\n",
    "\n",
    "for col_name in anime_train_columns:\n",
    "    if col_name not in anime_test_columns:\n",
    "        anime_test[str(col_name)] = 0\n",
    "        \n",
    "for col_name in anime_test_columns:\n",
    "    if col_name not in anime_train_columns:\n",
    "        anime_test.drop([str(col_name)], axis=1, inplace=True)\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1488, 7122)\n",
      "(138, 7122)\n"
     ]
    }
   ],
   "source": [
    "print(anime_train.shape)\n",
    "print(anime_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_y_train = anime_train['rating']\n",
    "anime_X_train = anime_train.drop(columns=['rating'])\n",
    "\n",
    "anime_y_test = anime_test['rating']\n",
    "anime_X_test = anime_test.drop(columns=['rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:299: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1488, 7121)\n",
      "(138, 7121)\n",
      "(1488, 700)\n",
      "(138, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression,k=700)#anime_X_test.shape[1]-1)\n",
    "features = selector.fit(anime_X_train, anime_y_train)\n",
    "\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "#print(fit.scores_)\n",
    "\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "anime_X_train = features.transform(anime_X_train)\n",
    "anime_X_test = features.transform(anime_X_test)\n",
    "print(anime_X_train.shape)\n",
    "print(anime_X_test.shape)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "anime_X_train = scaler.fit_transform(anime_X_train)  \n",
    "anime_X_test = scaler.transform(anime_X_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataset, label):\n",
    "    clf = model\n",
    "    clf.fit(dataset, label)\n",
    "    return clf\n",
    "\n",
    "def testing_evaluation(model, testset):\n",
    "    # Make predictions using the testing set\n",
    "    anime_y_pred = model.predict(testset)\n",
    "\n",
    "    # The mean absolute error\n",
    "    print(\"Mean absolute error: %.2f\" % np.sqrt(mean_absolute_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(anime_y_test, anime_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 2169853.15\n",
      "Mean squared error: 28028922659252.98\n",
      "Variance score: -786559122745666764737085440.00\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.LinearRegression(), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.82\n",
      "Mean squared error: 0.88\n",
      "Variance score: 0.23\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=0.1), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create a list of alphas to cross-validate against\\nalphas = np.logspace(-10, 1, 100)\\n\\n# Instantiate the linear model and visualizer\\nmodel = LassoCV(alphas=alphas, cv = 5)\\nvisualizer = AlphaSelection(model)\\n\\nvisualizer.fit(anime_X_train, anime_y_train)\\ng = visualizer.poof()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create a list of alphas to cross-validate against\n",
    "alphas = np.logspace(-10, 1, 100)\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = LassoCV(alphas=alphas, cv = 5)\n",
    "visualizer = AlphaSelection(model)\n",
    "\n",
    "visualizer.fit(anime_X_train, anime_y_train)\n",
    "g = visualizer.poof()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.958 0.823 0.933 0.947 1.078 0.952 0.991 0.772 1.085 0.938]\n",
      "Mean: 0.9477930278174469\n",
      "Standard deviation: 0.09193050937041902\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "scores = cross_val_score(clf, anime_X_train, anime_y_train, scoring=\"neg_mean_squared_error\", cv=10) \n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search For Hyper Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/sezin/.local/share/virtualenvs/dsa-DC4ML-_HMLejDF/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters are:  {'alpha': 0.01}\n",
      "The mean squared Error is: 0.87\n",
      "best estimator is:  Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "def checkHP(model, folds, dataset, label):\n",
    "    alphas = np.logspace(-5, 2, 30)\n",
    "    parameters = {\n",
    "                   \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  }\n",
    "\n",
    "    gd_sr = GridSearchCV(estimator=model,  \n",
    "                         param_grid=parameters,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=folds)\n",
    "\n",
    "    gd_sr.fit(dataset, label)  \n",
    "    \n",
    "    best_parameters = gd_sr.best_params_  \n",
    "    print(\"best parameters are: \", best_parameters)\n",
    "\n",
    "    best_result = gd_sr.best_score_  \n",
    "    print(\"The mean squared Error is: %.2f\" % -best_result) \n",
    "    best_estimator = gd_sr.best_estimator_\n",
    "    print(\"best estimator is: \", best_estimator) \n",
    "    \n",
    "checkHP(clf, 10, anime_X_train, anime_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.88\n",
      "Mean squared error: 1.00\n",
      "Variance score: -0.01\n"
     ]
    }
   ],
   "source": [
    "clf = training(model = linear_model.Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "   normalize=False, positive=False, precompute=False, random_state=None,\n",
    "   selection='cyclic', tol=0.0001, warm_start=False), dataset = anime_X_train, label= anime_y_train)\n",
    "testing_evaluation(clf, anime_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa-DC4ML",
   "language": "python",
   "name": "dsa-dc4ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
