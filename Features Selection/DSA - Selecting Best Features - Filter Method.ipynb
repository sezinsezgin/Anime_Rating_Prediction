{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the New Integrated Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imdb \n",
    "import pandas as pd\n",
    "anime_data = pd.read_csv(\"C://Users//Master//new_anime_data1.csv\")\n",
    "\n",
    "anime_data['episodes'] = anime_data['episodes'].replace('Unknown', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Some Special Characters from \"genre\" and \"overview\", and Covert \"type\" to Movie/Tv series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "\n",
    "\n",
    "anime_data['genre'].fillna('',  inplace=True)\n",
    "anime_data['overview'].fillna('',  inplace=True)\n",
    "anime_data['type'].fillna('',  inplace=True)\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['genre']\n",
    "    if(pd.isnull(item)):\n",
    "            item =\"\"\n",
    "    else:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            item = ','.join(item)\n",
    "        else:\n",
    "            item = item.replace(\" \",\"\")\n",
    "            item = item.replace(\"[\",\"\")\n",
    "            item = item.replace(\"]\",\"\")\n",
    "            item = item.replace(\"'\",\"\")\n",
    "    l1.append(item) \n",
    "      \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['overview']\n",
    "    if(pd.isnull(row['overview'])):\n",
    "            item = \"\"\n",
    "    else:\n",
    "       # for item in anime_data['overview']:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                item = ','.join(item)\n",
    "            else:\n",
    "                item = item.replace(\"[\",\"\")\n",
    "                item = item.replace(\"]\",\"\")\n",
    "    l2.append(item) \n",
    "\n",
    "for index, row in anime_data.iterrows():\n",
    "    item = row['type']\n",
    "    if(pd.isnull(row['type'])):\n",
    "        item = np.nan\n",
    "    else:    \n",
    "        if \"movie\" in item:\n",
    "            item = \"movie\"\n",
    "        else:\n",
    "            item = \"tv series\"\n",
    "    l3.append(item)   \n",
    "    \n",
    "    \n",
    "anime_data['genre'] = l1   \n",
    "anime_data['overview'] = l2\n",
    "anime_data['type'] = l3\n",
    "\n",
    "#drop dublicate\n",
    "#anime_data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying One-Hot and Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "anime_data = anime_data.dropna()\n",
    "\n",
    "#one-hot encoding to transform the genres column to numerical columns\n",
    "df = anime_data.genre.str.get_dummies(',')\n",
    "\n",
    "#binary encoding for type column\n",
    "type_lb = LabelBinarizer()\n",
    "X = type_lb.fit_transform(anime_data.type.values)\n",
    "\n",
    "\n",
    "dfOneHot = pd.DataFrame(X, columns = [\"movie/TVseries\" for i in range(X.shape[1])])\n",
    "anime_data = pd.concat([anime_data, dfOneHot], axis=1)\n",
    "anime_data = pd.concat([anime_data, df], axis=1)\n",
    "\n",
    "anime_data['movie/TVseries'].fillna(0, inplace=True)\n",
    "\n",
    "#anime_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words \"overview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "def get_words(x):\n",
    "    bagofwords=[]\n",
    "    for i in x:\n",
    "        if i[1]=='NN':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNP':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='NNPS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJ':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='JJS':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RB':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBR':\n",
    "            bagofwords.append(i[0])\n",
    "        elif i[1]=='RBS':\n",
    "            bagofwords.append(i[0])\n",
    "    return bagofwords\n",
    "\n",
    "def clean_words(x):\n",
    "    b=nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    result=get_words(b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "summary_doc = anime_data['overview'].fillna(\"\").map(clean_words)\n",
    "summary_doc =summary_doc.apply(','.join)\n",
    " \n",
    "vectorizer = TfidfVectorizer()\n",
    "overview_feature = vectorizer.fit_transform(summary_doc).toarray()\n",
    "#overview_feature = vectorizer.fit_transform(summary_doc)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(overview_feature, columns = [\"word\"+ str(int(i)) for i in range(overview_feature.shape[1])])\n",
    "anime_data = pd.concat([anime_data, df], axis=1)\n",
    "\n",
    "#drop Null values\n",
    "#anime_data = anime_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "overview_feature = vectorizer.fit_transform(anime_data['overview']).toarray()\n",
    "\n",
    "df = pd.DataFrame(overview_feature, columns = [\"word\"+ str(int(i)) for i in range(overview_feature.shape[1])])\n",
    "anime_data = pd.concat([anime_data, df], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anime_data = anime_data.drop(columns=['Unnamed: 0', 'anime_id', 'name', 'genre', 'overview', 'type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop None Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1930, 7743)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nanime_data.fillna(value=pd.np.nan, inplace=True)\\nimp = SimpleImputer(strategy=\"mean\")\\nfeature_vect = imp.fit_transform(anime_data)\\n\\ndf = pd.DataFrame(feature_vect, columns = [\"feature\"+ str(int(i)) for i in range(feature_vect.shape[1])])\\nanime_data = df\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "anime_data = anime_data.dropna()\n",
    "print(anime_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "y = anime_data['rating']\n",
    "X = anime_data.drop(columns=['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Filter Method - Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1930, 778)\n",
      "(1930, 7742)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=778)\n",
    "fit = selector.fit(X, y)\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "#print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "print(features.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "anime_X_train = features[:-500]\n",
    "anime_X_test = features[-500:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "anime_y_train = y[:-500]\n",
    "anime_y_test = y[-500:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(anime_X_train, anime_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "anime_y_pred = regr.predict(anime_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy: \n",
      " 6.4572730304813275\n",
      "Coefficients: \n",
      " [ 3.803e-04  7.100e-06 -1.516e-01  1.581e-02 -2.559e-02 -1.224e+00\n",
      " -1.509e+00  8.172e-03 -4.687e-01 -3.087e+00  1.644e+01 -6.510e-01\n",
      " -1.070e+01 -9.408e-11 -1.027e-10  3.600e-01  2.350e-01 -1.452e-01\n",
      "  1.709e+01 -1.016e-10 -3.600e-11 -4.687e-01 -1.452e-01  1.922e-11\n",
      " -1.452e-01 -4.471e+00 -1.271e+00 -1.416e-01 -7.231e-11 -7.544e-01\n",
      " -3.921e-01  7.453e-01 -1.815e-11 -1.456e-11  1.669e+00  4.713e+00\n",
      " -8.768e-01 -9.373e-01  8.064e-01  1.619e+00 -3.354e-11 -3.331e-01\n",
      " -7.965e-12 -3.921e-01 -4.687e-01  9.252e-01  1.488e+00 -3.399e-12\n",
      "  6.819e-01  4.982e-12 -1.093e-11  1.488e+00  1.488e+00  1.941e-11\n",
      " -7.246e+00 -4.693e-01  3.600e-01 -1.526e+00  2.334e+00 -3.623e+00\n",
      "  2.933e+00 -3.648e+00  1.526e-12 -1.455e-01 -1.575e+00  6.969e-01\n",
      "  1.800e-12 -1.452e-01  1.454e+01  4.141e+01 -4.500e-01 -2.832e-01\n",
      " -1.452e-01 -2.832e-01  1.046e+00  4.086e-02 -2.256e+01  2.482e-12\n",
      "  3.211e+00 -4.687e-01  1.003e+00  3.033e+00  9.610e-13  8.172e-03\n",
      " -4.687e-01  6.819e-01 -5.649e-13  8.064e-01  1.634e-02  3.600e-01\n",
      " -6.892e-13 -3.623e+00 -2.061e-13 -3.623e+00 -6.510e-01  1.175e-02\n",
      " -1.176e+00 -7.041e-01 -1.452e-01 -4.583e-13 -1.406e+01 -4.263e-13\n",
      " -4.237e-01 -1.922e+00 -7.544e-01 -6.386e+00 -1.244e+00  6.819e-01\n",
      "  3.662e+00 -6.510e-01  7.105e-14  3.020e-13 -9.311e+00  4.144e-01\n",
      "  1.821e+00  4.144e-01 -1.243e-14 -1.416e-01 -6.857e-01  8.809e-01\n",
      "  3.375e-14  1.488e+00 -7.628e+00 -7.994e-15 -3.554e+00 -7.544e-01\n",
      "  6.819e-01 -9.331e+00  1.977e+00 -7.544e-01  1.272e+01 -4.382e+00\n",
      " -2.531e-01  1.699e+00 -1.455e-01 -2.449e+00  1.313e+00 -1.416e-01\n",
      " -5.906e-14 -7.946e+01  1.088e-14  2.919e+00 -3.244e-01  9.902e+00\n",
      " -4.237e-01  1.733e+00 -3.373e-01 -1.416e-01 -1.865e-14 -3.912e+00\n",
      "  2.665e-14 -2.309e-14  6.735e-01 -1.452e-01  1.175e-02  3.600e-01\n",
      "  2.843e+00  8.739e-01  3.730e-14  5.674e-01 -6.510e-01  4.144e-01\n",
      " -8.714e+00 -7.187e-01  3.109e-15  1.818e+00 -3.623e+00  5.329e-15\n",
      "  1.488e+00 -1.452e-01  1.168e+00 -5.712e+00  7.568e+00 -7.544e-01\n",
      " -4.500e-01 -1.452e-01  4.144e-01  1.175e-02 -4.687e-01 -4.693e-01\n",
      "  1.837e+00 -8.768e-01  1.399e+00 -1.416e-01  1.769e-01 -1.452e-01\n",
      "  1.295e+00 -4.687e-01  3.524e-02 -9.178e+00  7.034e+00 -3.555e+00\n",
      "  1.771e-01 -6.510e-01  2.656e+00 -1.841e+00 -4.237e-01 -1.284e+00\n",
      " -3.623e+00 -5.465e-01 -1.922e+00 -9.770e-15  1.599e-14  4.704e-01\n",
      "  5.940e-15  1.295e+00 -2.407e+00 -3.074e+00 -9.373e-01 -4.086e-14\n",
      "  2.132e-14 -1.452e-01 -1.406e+00 -3.944e-01  8.064e-01 -1.110e-14\n",
      " -1.521e+00 -3.013e+00  7.895e+00 -4.235e+00 -3.232e+00 -2.186e+00\n",
      " -1.954e-14  8.064e-01  3.600e-01 -1.041e+01  1.687e+01 -8.768e-01\n",
      "  1.219e+00  2.665e-15  3.462e-01  4.144e-01  1.130e-02  5.773e-15\n",
      "  1.295e+00  3.600e-01  3.600e-01  0.000e+00  1.730e+00 -4.237e-01\n",
      " -1.815e+00 -6.910e-01  1.634e-02  6.523e+00  0.000e+00 -2.349e-01\n",
      "  1.175e-02  1.295e+00 -4.141e-01 -4.141e-01 -6.679e-01 -4.687e-01\n",
      "  1.488e+00 -8.944e+00 -1.452e-01 -3.623e+00 -4.687e-01  4.144e-01\n",
      " -7.265e-01  8.172e-03  3.600e-01 -4.237e-01  2.350e+00 -1.908e+01\n",
      " -6.510e-01 -3.445e+00 -9.308e+00 -1.990e+00  6.012e+00  3.215e+00\n",
      "  5.609e+00  2.762e+00  4.398e+00 -2.343e+00 -5.912e+00  3.990e-01\n",
      "  5.419e+00  0.000e+00 -1.416e-01  0.000e+00 -2.234e+00 -1.471e+00\n",
      " -5.628e-01 -1.253e+00 -1.281e+00  3.611e+00 -3.623e+00  1.371e+00\n",
      " -1.922e+00  1.175e-02 -9.373e-01  8.172e-03  1.088e+00 -3.120e+00\n",
      "  3.600e-01  1.080e+00  0.000e+00  0.000e+00  0.000e+00 -1.821e-01\n",
      " -1.821e-01 -4.284e-01 -4.143e-01  0.000e+00  7.076e+00  2.762e+00\n",
      "  1.175e-02 -7.265e-01  1.295e+00 -1.452e-01  7.453e-01 -4.500e-01\n",
      "  1.613e+00  8.064e-01 -8.768e-01  0.000e+00 -2.738e+00  7.199e-01\n",
      "  1.440e+00  3.600e-01  7.159e+00  0.000e+00 -3.642e-01  1.684e+00\n",
      " -9.896e-01 -4.368e+00  6.819e-01 -4.687e-01  3.600e-01 -7.265e-01\n",
      "  6.819e-01 -6.658e-01 -1.452e-01 -1.452e-01 -4.500e-01  1.769e-01\n",
      " -1.452e-01 -6.510e-01  3.600e-01  7.588e-01  7.453e-01  0.000e+00\n",
      "  2.592e+01  1.754e-01 -1.363e+00 -1.416e-01  8.172e-03 -1.416e-01\n",
      "  3.600e-01 -9.386e-01  0.000e+00  3.552e-01  0.000e+00  3.538e-01\n",
      "  7.199e-01 -1.452e-01  0.000e+00  7.194e+00  0.000e+00  8.952e-01\n",
      " -1.452e-01 -1.907e+00 -9.035e-01 -1.452e-01  0.000e+00  4.144e-01\n",
      " -1.452e-01 -6.125e-01  0.000e+00 -1.922e+00 -1.364e+01  9.252e-01\n",
      "  6.677e+00  1.320e+00  5.674e-01 -3.921e-01 -4.500e-01  2.419e+00\n",
      "  4.086e-02 -9.241e-01 -7.544e-01  0.000e+00  2.735e+00  4.144e-01\n",
      " -1.821e-01  0.000e+00  7.199e-01  2.091e+00  9.252e-01  8.172e-03\n",
      " -1.177e+00 -4.687e-01 -3.623e+00 -2.555e+00  8.366e-01  0.000e+00\n",
      "  0.000e+00 -1.452e-01 -1.737e+01 -1.363e+00  8.489e-01  6.217e+00\n",
      " -4.687e-01 -3.982e-01  8.172e-03 -1.492e+00  1.344e+00 -6.510e-01\n",
      " -2.929e+00 -1.113e+01  2.286e+00  6.819e-01  0.000e+00  0.000e+00\n",
      "  1.392e+00  0.000e+00 -4.687e-01  0.000e+00  3.600e-01 -5.483e+00\n",
      "  0.000e+00  1.320e+01  3.600e-01  0.000e+00  6.819e-01 -4.368e+00\n",
      "  0.000e+00  7.199e-01 -1.897e+00  0.000e+00  1.175e-02  0.000e+00\n",
      "  0.000e+00  4.218e+00  6.401e-01 -3.623e+00 -5.463e-01 -4.141e-01\n",
      "  2.001e+00 -7.448e+00  1.769e-01  0.000e+00 -3.373e-01  2.637e+00\n",
      "  5.133e+00 -8.768e-01  0.000e+00 -1.416e-01  0.000e+00 -1.012e+00\n",
      "  6.819e-01 -9.373e-01 -6.510e-01  0.000e+00  0.000e+00  0.000e+00\n",
      " -4.237e-01  2.157e+00  4.398e+00 -1.008e+00  6.538e-02  2.621e+00\n",
      " -1.452e-01  7.577e-01 -4.693e-01  4.398e+00  1.371e+00  0.000e+00\n",
      " -1.455e-01  0.000e+00  0.000e+00  0.000e+00  1.634e-02  9.606e+00\n",
      " -2.555e+00 -5.936e-01  0.000e+00  0.000e+00  3.600e-01  1.702e+00\n",
      "  6.470e-01 -1.452e-01 -3.702e+00  0.000e+00 -1.416e-01 -2.956e+00\n",
      "  0.000e+00  0.000e+00 -1.416e-01 -4.693e-01 -3.642e-01 -3.585e-01\n",
      " -2.832e-01  0.000e+00  3.462e-01  0.000e+00  7.199e-01  8.064e-01\n",
      "  1.264e+01 -4.687e-01 -1.452e-01 -5.822e+00  5.674e-01 -2.078e+01\n",
      " -4.777e+00  0.000e+00 -3.732e-01  0.000e+00 -3.373e-01 -1.363e+00\n",
      " -2.832e-01 -2.328e+00  1.431e+00  0.000e+00  4.144e-01  3.600e-01\n",
      "  3.600e-01  7.453e-01  5.898e+00  0.000e+00  1.929e+00 -6.510e-01\n",
      " -6.510e-01 -5.332e+01  2.091e+00 -2.357e+00  9.807e+00 -2.343e+00\n",
      "  0.000e+00  5.741e+00  2.013e+00  2.250e-01  9.359e-01  1.364e+00\n",
      " -3.990e+00  0.000e+00 -2.175e-01  5.968e+00  0.000e+00 -1.773e+01\n",
      " -5.041e+00  0.000e+00  0.000e+00 -2.343e+00 -1.416e-01  0.000e+00\n",
      "  1.445e+00  9.654e+00  0.000e+00  0.000e+00  0.000e+00 -3.749e+00\n",
      "  0.000e+00  2.145e+00  9.252e-01 -1.452e-01 -4.141e-01  1.634e-02\n",
      "  3.591e-01 -7.265e-01 -1.493e+00  9.160e+00  0.000e+00  0.000e+00\n",
      "  0.000e+00 -1.416e-01  5.759e-01  3.984e+00 -5.463e-01  3.600e-01\n",
      " -4.687e-01  0.000e+00 -1.413e+00  7.453e-01  9.886e-01  0.000e+00\n",
      " -1.452e-01 -1.526e+00  2.052e+00  4.493e+00  7.970e-01  0.000e+00\n",
      " -1.452e-01  0.000e+00  1.632e-01 -6.555e-02 -8.768e-01  1.825e+00\n",
      "  0.000e+00  0.000e+00 -1.416e-01 -1.416e-01  0.000e+00 -6.510e-01\n",
      "  0.000e+00  2.160e+00  5.674e-01  1.908e+00  3.600e-01  6.819e-01\n",
      " -6.510e-01  0.000e+00  7.453e-01  0.000e+00 -8.736e+00 -4.368e+00\n",
      " -1.416e-01  3.600e-01  0.000e+00 -6.510e-01  3.600e-01 -7.090e+00\n",
      " -1.179e+00  1.488e+00  2.590e+00  4.046e+01  0.000e+00 -7.544e-01\n",
      "  0.000e+00  3.600e-01  0.000e+00 -3.844e+00  8.172e-03 -1.119e+00\n",
      " -1.922e+00 -4.237e-01  7.453e-01  0.000e+00  6.596e-01  0.000e+00\n",
      " -1.660e+00 -1.622e-01  8.064e-01  0.000e+00  0.000e+00  1.371e+00\n",
      " -4.317e+00 -3.373e-01  7.453e-01 -8.955e+00 -1.416e-01  4.144e-01\n",
      "  6.504e-01 -1.452e-01 -4.237e-01  3.693e+00 -1.606e+01  2.074e+00\n",
      "  2.562e+00 -7.544e-01 -2.782e+00  5.328e+00  2.827e+00  8.064e-01\n",
      "  7.076e+00 -1.671e+00 -1.026e+01  2.762e+00  3.473e+00  3.102e+00\n",
      " -1.452e-01  0.000e+00  3.600e-01 -1.922e+00  1.293e+00 -1.179e+00\n",
      " -1.452e-01 -8.799e+00 -1.452e-01  9.814e-01 -7.544e-01 -1.452e-01\n",
      "  0.000e+00  0.000e+00 -1.452e-01  7.199e-01 -1.416e-01  2.578e+00\n",
      "  7.769e-01 -4.687e-01 -1.987e+01  0.000e+00  0.000e+00 -1.452e-01\n",
      " -2.555e+00  0.000e+00 -1.853e+01  1.371e+00  1.685e+01 -1.452e-01\n",
      "  2.183e+00 -1.416e-01  2.091e+00  1.630e+01 -3.397e+00  0.000e+00\n",
      " -4.368e+00  0.000e+00 -4.693e-01 -4.693e-01  0.000e+00  0.000e+00\n",
      "  0.000e+00 -4.560e-01  8.172e-03  2.095e+00 -9.197e-01  6.800e+00\n",
      " -6.510e-01 -7.294e-01  8.478e-01  0.000e+00 -6.510e-01 -1.008e+01\n",
      " -4.141e-01 -7.265e-01  6.819e-01  4.879e-01 -7.965e-01  1.295e+00\n",
      "  0.000e+00 -3.807e-01 -7.265e-01 -2.832e-01  0.000e+00  3.600e-01\n",
      " -1.105e+01  5.674e-01  7.199e-01 -4.687e-01 -3.623e+00 -3.623e+00\n",
      "  3.600e-01 -2.910e+00 -5.820e-01  0.000e+00 -8.389e+00 -7.162e-01\n",
      " -4.687e-01 -1.416e-01  0.000e+00 -4.237e-01  3.684e+00  0.000e+00\n",
      "  3.782e+00  0.000e+00 -4.687e-01 -1.706e+01  1.518e+00 -1.179e+00\n",
      " -5.202e+00  3.600e-01  3.600e-01  1.104e+01  2.166e+00  3.207e+00\n",
      " -4.122e-01  0.000e+00  8.569e+00 -5.357e+00  0.000e+00  1.135e+00\n",
      "  1.180e+01 -8.768e-01  0.000e+00  1.440e+00  1.422e+00  0.000e+00\n",
      "  0.000e+00 -1.452e-01 -5.937e+00  3.600e-01 -8.768e-01 -1.416e-01\n",
      "  2.071e-01 -1.179e+00 -6.510e-01 -6.510e-01]\n",
      "Mean squared error: 1.62\n",
      "Variance score: -1.49\n"
     ]
    }
   ],
   "source": [
    "#accurcy\n",
    "print('Accurcy: \\n', regr.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(anime_y_test, anime_y_pred)))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(anime_y_test, anime_y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
